{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAI, ChatGoogleGenerativeAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import hashlib\n",
    "\n",
    "from gptcache import Cache\n",
    "from gptcache.manager.factory import manager_factory\n",
    "from gptcache.manager import CacheBase\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from langchain.cache import GPTCache\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashed_name(name: str):\n",
    "    return hashlib.sha256(name.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_base = CacheBase(name='sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gptcache(cache_obj: Cache, llm: str):\n",
    "    hashed_llm = get_hashed_name(llm)\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func=get_prompt,\n",
    "        data_manager=manager_factory(manager=cache_base, max_size=10_000),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_llm_cache(GPTCache(init_gptcache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0, max_tokens=2048)\n",
    "openai_chat = ChatOpenAI(model='gpt-3.5-turbo-1106', temperature=0, max_tokens=2048)\n",
    "gemini_llm = GoogleGenerativeAI(model='gemini-pro', temperature=0, max_output_tokens=2048)\n",
    "gemini_chat = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0, max_output_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm.invoke('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_chat.invoke('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_llm.invoke('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_chat.invoke('Hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_providers = [\n",
    "    \"openai\",\n",
    "    \"google\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = {\n",
    "    \"openai\": {\n",
    "        \"base\": [\n",
    "            \"gpt-3.5-turbo-instruct\"\n",
    "        ],\n",
    "        \"chat\": [\n",
    "            \"gpt-3.5-turbo-16k\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-3.5-turbo-1106\",\n",
    "            \"gpt-4-32k-0613\",\n",
    "            \"gpt-4-0613\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4\",\n",
    "            \"gpt-4-vision-preview\",\n",
    "            \"gpt-4-1106-preview\",\n",
    "        ]\n",
    "    },\n",
    "    \"google\": {\n",
    "        \"base\": [\n",
    "            \"gemini-pro\",\n",
    "            \"text-bison-001\",\n",
    "            \"text-bison-002\",\n",
    "        ],\n",
    "        \"chat\": [\n",
    "            \"gemini-pro\",\n",
    "            \"chat-bison-001\",\n",
    "            \"chat-bison-002\",\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0\n",
    "MAX_TOKENS = 2048\n",
    "set_llm_cache(SQLiteCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for provider in llm_providers:\n",
    "    for model_type in llm_model[provider]:\n",
    "        for model in llm_model[provider][model_type]:\n",
    "            print(f'{provider}-{model}')\n",
    "            if provider == 'openai':\n",
    "                if model_type == 'base':\n",
    "                    models[model] = OpenAI(model=model, temperature=TEMPERATURE, max_tokens=MAX_TOKENS)\n",
    "                elif model_type == 'chat':\n",
    "                    models[model] = ChatOpenAI(model=model, temperature=TEMPERATURE, max_tokens=MAX_TOKENS)\n",
    "            elif provider == 'google':\n",
    "                if model_type == 'base':\n",
    "                    models[model] = GoogleGenerativeAI(model=model, temperature=TEMPERATURE, max_output_tokens=MAX_TOKENS)\n",
    "                elif model_type == 'chat':\n",
    "                    models[f\"{model} - chatmodel\"] = ChatGoogleGenerativeAI(model=model, temperature=TEMPERATURE, max_output_tokens=MAX_TOKENS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['gemini-pro'].invoke('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['gemini-pro - chatmodel'].invoke('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['text-bison-001'].invoke('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['text-bison-002'].invoke('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['text-bison-002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI, HarmCategory, HarmBlockThreshold, VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    template=\"\"\"Generate {{ k }} questions that test reader comprehension of the following text.\n",
    "\n",
    "Text: {{ text }}\n",
    "\n",
    "Questions:\"\"\",\n",
    "    template_format=\"jinja2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"Translate this sentence from English to French. I love programming.\"\n",
    "messages = [HumanMessage(content=human)]\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "chat = ChatVertexAI(temperature=1, max_output_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAI(model='text-bison-002', temperature=1, max_output_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.generate([messages], max_output_tokens=1).generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(prompt.format(**{\"k\":1, \"text\": \"Hello world\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.generate([{\"k\":1, \"text\": \"Hello world\"}]).generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
